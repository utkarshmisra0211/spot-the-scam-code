{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\utkar\\Desktop\\hackathon_goa\\myenv\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\utkar\\Desktop\\hackathon_goa\\myenv\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001BA3B56E0E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001BA3B56E0E0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
      "\n",
      "Analysis Results:\n",
      "SSL Score: 100.00%\n",
      "URL Score: 69.05%\n",
      "Content Score: 66.73%\n",
      "Image Score: 70.81%\n",
      "Overall Score: 63.20%\n",
      "\n",
      "Text Analysis:\n",
      "- The page contains a form, which could be used to collect sensitive information.\n",
      "- Found 1 suspicious keywords that are common in phishing attempts.\n",
      "- The page asks users to update or verify information, a common phishing tactic.\n",
      "\n",
      "Image Analysis:\n",
      "- The image contains a large amount of text, which is common in phishing attempts.\n",
      "\n",
      "Verdict: The website is likely legitimate.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\utkar\\AppData\\Local\\Temp\\ipykernel_13372\\2527748329.py:242: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  feedback_data = pd.concat([feedback_data, new_data], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not enough data to train the model yet. Using simple heuristics for now.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001BA3AF103A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x000001BA3AF103A0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
      "\n",
      "Analysis Results:\n",
      "SSL Score: 100.00%\n",
      "URL Score: 94.54%\n",
      "Content Score: 66.00%\n",
      "Image Score: 82.97%\n",
      "Overall Score: 71.15%\n",
      "\n",
      "Text Analysis:\n",
      "- No specific suspicious elements found in the text content.\n",
      "\n",
      "Image Analysis:\n",
      "- No specific suspicious elements found in the image.\n",
      "\n",
      "Verdict: The website is likely legitimate.\n",
      "Model updated. Current accuracy: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\n",
      "Analysis Results:\n",
      "SSL Score: 100.00%\n",
      "URL Score: 75.08%\n",
      "Content Score: 50.95%\n",
      "Image Score: 72.17%\n",
      "Overall Score: 60.80%\n",
      "\n",
      "Text Analysis:\n",
      "- No specific suspicious elements found in the text content.\n",
      "\n",
      "Image Analysis:\n",
      "- The image contains a large amount of text, which is common in phishing attempts.\n",
      "\n",
      "Verdict: The website is likely legitimate.\n",
      "Model updated. Current accuracy: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
      "Content analysis error: HTTPSConnectionPool(host='netflix-billing.com', port=443): Max retries exceeded with url: /index.php (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1007)')))\n",
      "\n",
      "Analysis Results:\n",
      "SSL Score: 0.00%\n",
      "URL Score: 52.37%\n",
      "Content Score: 0.00%\n",
      "Image Score: 46.53%\n",
      "Overall Score: 22.11%\n",
      "\n",
      "Text Analysis:\n",
      "- Unable to analyze content\n",
      "\n",
      "Image Analysis:\n",
      "- The image contains a large amount of text, which is common in phishing attempts.\n",
      "- The image doesn't strongly resemble common website layouts.\n",
      "\n",
      "Verdict: The website is highly likely fraudulent.\n",
      "Model updated. Current accuracy: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\n",
      "Analysis Results:\n",
      "SSL Score: 100.00%\n",
      "URL Score: 65.88%\n",
      "Content Score: 90.27%\n",
      "Image Score: 81.06%\n",
      "Overall Score: 71.01%\n",
      "\n",
      "Text Analysis:\n",
      "- The page asks users to update or verify information, a common phishing tactic.\n",
      "\n",
      "Image Analysis:\n",
      "- No specific suspicious elements found in the image.\n",
      "\n",
      "Verdict: The website is likely legitimate.\n",
      "Model updated. Current accuracy: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step\n",
      "\n",
      "Analysis Results:\n",
      "SSL Score: 100.00%\n",
      "URL Score: 73.65%\n",
      "Content Score: 87.15%\n",
      "Image Score: 55.30%\n",
      "Overall Score: 65.34%\n",
      "\n",
      "Text Analysis:\n",
      "- No specific suspicious elements found in the text content.\n",
      "\n",
      "Image Analysis:\n",
      "- The image contains a large amount of text, which is common in phishing attempts.\n",
      "\n",
      "Verdict: The website is likely legitimate.\n",
      "Model updated. Current accuracy: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
      "Content analysis error: HTTPSConnectionPool(host='www.facebookssecuritycenter.com', port=443): Max retries exceeded with url: /index.html/ (Caused by SSLError(SSLCertVerificationError(1, '[SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: self-signed certificate in certificate chain (_ssl.c:1007)')))\n",
      "\n",
      "Analysis Results:\n",
      "SSL Score: 0.00%\n",
      "URL Score: 79.45%\n",
      "Content Score: 0.00%\n",
      "Image Score: 81.37%\n",
      "Overall Score: 36.23%\n",
      "\n",
      "Text Analysis:\n",
      "- Unable to analyze content\n",
      "\n",
      "Image Analysis:\n",
      "- No specific suspicious elements found in the image.\n",
      "\n",
      "Verdict: The website is highly likely fraudulent.\n",
      "Model updated. Current accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import ssl\n",
    "import socket\n",
    "from urllib.parse import urlparse\n",
    "import torch\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow import keras\n",
    "from extract import extract_features\n",
    "from feature import *\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "global feedback_data\n",
    "feedback_data = pd.DataFrame(columns=['ssl_score', 'url_score', 'content_score','image_score', 'overall_score', 'label'])\n",
    "\n",
    "\n",
    "# Load the Malicious URL Prediction model\n",
    "model_path = r\"./model/Malicious_URL_Prediction.h5\"\n",
    "\n",
    "# Initialize DistilBERT model and tokenizer\n",
    "nlp_tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "nlp_model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# Initialize ResNet model for image analysis\n",
    "image_model = models.resnet50(pretrained=True)\n",
    "image_model.eval()\n",
    "\n",
    "# Image preprocessing transformations\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Initialize feedback data storage and classification model\n",
    "feedback_data = pd.DataFrame(columns=['ssl_score', 'url_score', 'content_score', \n",
    "                                      'image_score', 'overall_score', 'label'])\n",
    "classification_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "def check_ssl(url):\n",
    "    try:\n",
    "        hostname = urlparse(url).hostname\n",
    "        context = ssl.create_default_context()\n",
    "        with socket.create_connection((hostname, 443)) as sock:\n",
    "            with context.wrap_socket(sock, server_hostname=hostname) as ssock:\n",
    "                return 100  # SSL valid\n",
    "    except Exception:\n",
    "        return 0  # SSL invalid\n",
    "\n",
    "def get_prediction(url, model_path):\n",
    "    model = keras.models.load_model(model_path)\n",
    "    url_features = extract_features(url)\n",
    "    url_features_array = np.array([url_features])\n",
    "    prediction = model.predict(url_features_array)\n",
    "    return round(prediction[0][0] * 100, 3)\n",
    "\n",
    "def analyze_content(url):\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        text_content = ' '.join(soup.stripped_strings)\n",
    "\n",
    "        inputs = nlp_tokenizer(text_content, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
    "        with torch.no_grad():\n",
    "            outputs = nlp_model(**inputs)\n",
    "        probabilities = torch.softmax(outputs.logits, dim=1)\n",
    "        content_similarity = probabilities[0][1].item()\n",
    "\n",
    "        form_presence = 1 if soup.find('form') else 0\n",
    "\n",
    "        suspicious_keywords = ['login', 'password', 'credit card', 'social security', 'urgent', 'verify']\n",
    "        keyword_count = sum(keyword in text_content.lower() for keyword in suspicious_keywords)\n",
    "        keyword_score = max(0, 1 - keyword_count / len(suspicious_keywords))\n",
    "\n",
    "        # Improved content score calculation\n",
    "        content_length = len(text_content)\n",
    "        length_score = min(1, content_length / 1000)  # Assume 1000 characters is a good length\n",
    "        link_count = len(soup.find_all('a'))\n",
    "        link_score = min(1, link_count / 20)  # Assume 20 links is a good number\n",
    "\n",
    "        content_score = (content_similarity + (1 - form_presence) + keyword_score + length_score + link_score) / 5 * 100\n",
    "        \n",
    "        text_analysis = []\n",
    "        if form_presence:\n",
    "            text_analysis.append(\"The page contains a form, which could be used to collect sensitive information.\")\n",
    "        if keyword_count > 0:\n",
    "            text_analysis.append(f\"Found {keyword_count} suspicious keywords that are common in phishing attempts.\")\n",
    "        if 'https' in text_content.lower():\n",
    "            text_analysis.append(\"The page mentions 'https', which could be an attempt to appear secure.\")\n",
    "        if 'update' in text_content.lower() or 'verify' in text_content.lower():\n",
    "            text_analysis.append(\"The page asks users to update or verify information, a common phishing tactic.\")\n",
    "        if not text_analysis:\n",
    "            text_analysis.append(\"No specific suspicious elements found in the text content.\")\n",
    "\n",
    "        return content_score, text_analysis\n",
    "    except Exception as e:\n",
    "        print(f\"Content analysis error: {e}\")\n",
    "        return 0, [\"Unable to analyze content\"]\n",
    "\n",
    "def analyze_image(image_path):\n",
    "    try:\n",
    "        img = Image.open(image_path).convert('RGB')\n",
    "        img_t = image_transforms(img)\n",
    "        batch_t = torch.unsqueeze(img_t, 0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = image_model(batch_t)\n",
    "\n",
    "        probabilities = torch.nn.functional.softmax(out[0], dim=0)\n",
    "        general_score = probabilities.max().item()\n",
    "\n",
    "        img_cv = cv2.imread(image_path)\n",
    "        gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        edges = cv2.Canny(gray, 100, 200)\n",
    "        edge_ratio = np.sum(edges > 0) / edges.size\n",
    "\n",
    "        _, thresh = cv2.threshold(gray, 150, 255, cv2.THRESH_BINARY_INV)\n",
    "        kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (18, 18))\n",
    "        dilation = cv2.dilate(thresh, kernel, iterations=1)\n",
    "        contours, _ = cv2.findContours(dilation, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "        text_regions = sum(cv2.contourArea(c) for c in contours)\n",
    "        text_ratio = text_regions / (img_cv.shape[0] * img_cv.shape[1])\n",
    "\n",
    "        orb = cv2.ORB_create()\n",
    "        kp = orb.detect(img_cv, None)\n",
    "        logo_score = len(kp) / 1000\n",
    "\n",
    "        image_score = (general_score + (1 - edge_ratio) + (1 - text_ratio) + logo_score) / 4 * 100\n",
    "\n",
    "        image_analysis = []\n",
    "        if edge_ratio > 0.1:\n",
    "            image_analysis.append(\"The image contains many edges, which could indicate a complex or poorly designed interface.\")\n",
    "        if text_ratio > 0.3:\n",
    "            image_analysis.append(\"The image contains a large amount of text, which is common in phishing attempts.\")\n",
    "        if logo_score < 0.1:\n",
    "            image_analysis.append(\"No clear logo detected, which is unusual for legitimate websites.\")\n",
    "        if general_score < 0.5:\n",
    "            image_analysis.append(\"The image doesn't strongly resemble common website layouts.\")\n",
    "        if not image_analysis:\n",
    "            image_analysis.append(\"No specific suspicious elements found in the image.\")\n",
    "\n",
    "        return image_score, image_analysis\n",
    "    except Exception as e:\n",
    "        print(f\"Image analysis error: {e}\")\n",
    "        return 0, [\"Unable to analyze image\"]\n",
    "\n",
    "def calculate_overall_score(scores):\n",
    "    weights = {\n",
    "        'ssl_score': 0.15,\n",
    "        'url_score': 0.2,\n",
    "        'content_score': 0.25,\n",
    "        'image_score': 0.25\n",
    "    }\n",
    "    overall_score = sum(score * weights[key] for key, score in scores.items())\n",
    "    return overall_score\n",
    "\n",
    "def train_model():\n",
    "    global classification_model, feedback_data\n",
    "    if len(feedback_data) > 1:\n",
    "        X = feedback_data.drop('label', axis=1)\n",
    "        y = feedback_data['label']\n",
    "\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(y)\n",
    "\n",
    "        classification_model.fit(X, y)\n",
    "        y_pred = classification_model.predict(X)\n",
    "        accuracy = accuracy_score(y, y_pred)\n",
    "        print(f\"Model updated. Current accuracy: {accuracy:.2f}\")\n",
    "        if len(feedback_data) > 10:\n",
    "            print(classification_report(y, y_pred))\n",
    "    else:\n",
    "        print(\"Not enough data to train the model yet. Using simple heuristics for now.\")\n",
    "\n",
    "def analyze_website(url, image_path):\n",
    "    ssl_score = check_ssl(url)\n",
    "    url_score = get_prediction(url, model_path)\n",
    "    content_score, text_analysis = analyze_content(url)\n",
    "    image_score, image_analysis = analyze_image(image_path)\n",
    "\n",
    "    scores = {\n",
    "        'ssl_score': ssl_score,\n",
    "        'url_score': 100-url_score,\n",
    "        'content_score': content_score,\n",
    "        'image_score': image_score\n",
    "    }\n",
    "\n",
    "    overall_score = calculate_overall_score(scores)\n",
    "\n",
    "    return scores, overall_score, text_analysis, image_analysis\n",
    "\n",
    "def get_verdict(overall_score):\n",
    "    if overall_score >= 80:\n",
    "        return \"The website is highly likely legitimate.\"\n",
    "    elif 55 <= overall_score < 80:\n",
    "        return \"The website is likely legitimate.\"\n",
    "    elif 40 <= overall_score < 55:\n",
    "        return \"The website is likely fraudulent.\"\n",
    "    else:\n",
    "        return \"The website is highly likely fraudulent.\"\n",
    "\n",
    "\n",
    "def main():\n",
    "    global feedback_data\n",
    "    while True:\n",
    "        url = input(\"Enter the URL to analyze (or 'quit' to exit): \")\n",
    "        if url.lower() == 'quit':\n",
    "            break\n",
    "        image_path = input(\"Enter the path to the screenshot image: \")\n",
    "\n",
    "        scores, overall_score, text_analysis, image_analysis = analyze_website(url, image_path)\n",
    "\n",
    "        print(\"\\nAnalysis Results:\")\n",
    "        print(f\"SSL Score: {scores['ssl_score']:.2f}%\")\n",
    "        print(f\"URL Score: {scores['url_score']:.2f}%\")\n",
    "        print(f\"Content Score: {scores['content_score']:.2f}%\")\n",
    "        print(f\"Image Score: {scores['image_score']:.2f}%\")\n",
    "        print(f\"Overall Score: {overall_score:.2f}%\\n\")\n",
    "\n",
    "        print(\"Text Analysis:\")\n",
    "        for analysis in text_analysis:\n",
    "            print(f\"- {analysis}\")\n",
    "\n",
    "        print(\"\\nImage Analysis:\")\n",
    "        for analysis in image_analysis:\n",
    "            print(f\"- {analysis}\")\n",
    "\n",
    "        verdict = get_verdict(overall_score)\n",
    "        print(f\"\\nVerdict: {verdict}\")\n",
    "\n",
    "        feedback = input(\"\\nWas the prediction correct? (yes/no): \")\n",
    "        new_data = pd.DataFrame([{**scores, 'overall_score': overall_score, 'label': 'correct' if feedback.lower() == 'yes' else 'incorrect'}])\n",
    "        feedback_data = pd.concat([feedback_data, new_data], ignore_index=True)\n",
    "\n",
    "        train_model()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
